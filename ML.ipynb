{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fdd8b65",
   "metadata": {},
   "source": [
    "# Thesis Code Part 3: ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc1c01",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scispacy\n",
    "import spacy\n",
    "from negspacy.negation import Negex\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# SpaCy model for biomedical processing\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "nlp.add_pipe(\"negex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ca05b",
   "metadata": {},
   "source": [
    "## Acquire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_val_dataset.csv')\n",
    "test = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe4bba",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Chen & Sohn\n",
    "# https://colab.research.google.com/drive/1jp8Oi2s13g2B34SPjX5074FDBlhmUdgn?usp=sharing#scrollTo=MIA9a7rckKil\n",
    "def preprocess(nlp_model,input_text):\n",
    "    input_text = input_text.strip()\n",
    "    doc = nlp_model(input_text)\n",
    "    negation_list = [0]*len(doc)\n",
    "    tokens = list()\n",
    "    stop = set(stopwords.words('english')+list(string.punctuation))\n",
    "    stop.add(\"XXXX\")\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent._.negex:\n",
    "            index = ent.start\n",
    "            while index < ent.end:\n",
    "                negation_list[index] = 1\n",
    "                index += 1\n",
    "        \n",
    "    for i,token in enumerate(doc):\n",
    "        if str(token).lower() not in stop:\n",
    "            if negation_list[i] == 1:\n",
    "                tokens.append((\"NEGEX_\"+str(token).lower()))\n",
    "            else:\n",
    "                tokens.append(str(token).lower())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9674ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "train_text = train_df[\"full-text\"].to_list()\n",
    "\n",
    "def token_generator(text_list):\n",
    "    for text in text_list:\n",
    "        yield preprocess(nlp,text)\n",
    "\n",
    "train_tokens = token_generator(train_text)\n",
    "train_vocab_dict = Dictionary(train_tokens)\n",
    "\n",
    "# gensim Dictionary does not create an id2token dictionary  until dictionary is called\n",
    "train_vocab_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_df[\"full-text\"].to_list()\n",
    "test_tokens = token_generator(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37c508",
   "metadata": {},
   "source": [
    "## Modeling - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_vector_create(tuple_list,vocab_len):\n",
    "    #tuple_list will have data structure akin to gensim dictionary doc2bow output \n",
    "    sparse_vector = np.zeros(vocab_len)\n",
    "    for id,freq in tuple_list:\n",
    "        sparse_vector[id] = freq\n",
    "    return sparse_vector\n",
    "\n",
    "def sparse_vector_generator(tokens,vocab_dict,vocab_len):\n",
    "    for token in tokens:\n",
    "        yield sparse_vector_create(vocab_dict.doc2bow(token),vocab_len)\n",
    "\n",
    "# recreate generator object to reset it , otherwise will output empty result\n",
    "train_tokens = token_generator(train_text)\n",
    "x_train_sparse = [sparse_vector for sparse_vector in sparse_vector_generator(train_tokens,train_vocab_dict,len(train_vocab_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b15a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "nb_classifier = naive_bayes.MultinomialNB(alpha=1.0)\n",
    "\n",
    "nb_classifier.fit(x_train_sparse,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44907a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = token_generator(test_text)\n",
    "nb_predictions = list()\n",
    "\n",
    "for token in test_tokens:\n",
    "    test_sparse_vector = sparse_vector_create(train_vocab_dict.doc2bow(token),len(train_vocab_dict))\n",
    "    nb_predictions.append(nb_classifier.predict(test_sparse_vector.reshape(1,-1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcd6ec",
   "metadata": {},
   "source": [
    "## Modeling - Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, smooth_idf = True,sublinear_tf = True)\n",
    "tfidf.fit(x_train_sparse)\n",
    "x_train_tfidf= tfidf.transform(x_train_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(C=1.0,kernel ='linear')\n",
    "SVM.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = token_generator(test_text)\n",
    "svm_predictions = list()\n",
    "\n",
    "for token in test_tokens:\n",
    "    test_sparse_vector = sparse_vector_create(train_vocab_dict.doc2bow(token),len(train_vocab_dict))\n",
    "    x_test_tfidf = tfidf.transform(test_sparse_vector.reshape(1,-1))\n",
    "    svm_predictions.append(SVM.predict(x_test_tfidf)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707d657",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be946f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "class metric_calc:\n",
    "    def __init__(self, y,y_hat):\n",
    "        # y is true label, y_hat is predicted label\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        conf_matrix = metrics.confusion_matrix(y,y_hat)\n",
    "    \n",
    "        self.true_neg = conf_matrix[0][0]\n",
    "        self.false_pos = conf_matrix[0][1]\n",
    "        self.false_neg = conf_matrix[1][0]\n",
    "        self.true_pos = conf_matrix[1][1]\n",
    "        self.sum = self.true_pos + self.true_neg + self.false_pos + self.false_neg\n",
    "\n",
    "    def conf_matrix_values(self):\n",
    "        print(f\"TN: {self.true_neg}, FP: {self.false_pos}, FN: {self.false_neg} TP: {self.true_pos}\")\n",
    "    \n",
    "    def accuracy(self):\n",
    "        acc = (self.true_pos + self.true_neg)/self.sum\n",
    "        print(f\"Accuracy is {acc:.4f}\")\n",
    "\n",
    "    def sens_spec(self):\n",
    "        sens = self.true_pos/(self.true_pos+self.false_neg)\n",
    "        spec = self.true_neg/(self.true_neg+self.false_pos)\n",
    "        print(f\"Sensitivity (aka recall) is {sens:.4f}\")\n",
    "        print(f\"Specificity is {spec:.4f}\")\n",
    "\n",
    "    def incorrect_index(self):\n",
    "        # return indices of examples that are incorrectly predicted\n",
    "        index = list()\n",
    "        for i,label in enumerate(self.y):\n",
    "            if label != self.y_hat[i]:\n",
    "                index.append(i)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes\")\n",
    "nb_metric = metric_calc(y_test,nb_predictions)\n",
    "nb_metric.conf_matrix_values()\n",
    "nb_metric.accuracy()\n",
    "nb_metric.sens_spec()\n",
    "\n",
    "print(\"SVM\")\n",
    "svm_metric = metric_calc(y_test,svm_predictions)\n",
    "svm_metric.conf_matrix_values()\n",
    "svm_metric.accuracy()\n",
    "svm_metric.sens_spec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
